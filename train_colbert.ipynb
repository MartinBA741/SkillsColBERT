{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "#df['target'] = 1  # set target = 1, i.e. correct match\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Assign wrong documents to queries by spliting df in 2 halfs\n",
    "#split_index = round(len(df)/2) \n",
    "#df_fail = pd.DataFrame()\n",
    "#df_fail['query'] = df['query'].copy()\n",
    "#df_fail['documents'] = str()\n",
    "#df_fail['documents'].iloc[:split_index] = df['documents'].iloc[split_index+1:].copy()\n",
    "#df_fail['documents'].iloc[split_index:] = df['documents'].iloc[:split_index+1].copy()\n",
    "#df_fail['target'] = 0 # set target = 0, i.e. not a correct match\n",
    "\n",
    "## Append dataframes \n",
    "#df = df.append(df_fail, ignore_index=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu. Be patient...\n"
     ]
    }
   ],
   "source": [
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device==\"cuda\":\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "    print(f'Running on {device} with {n_gpu} number of GPUs')\n",
    "else:\n",
    "    print(f'Running on {device}. Be patient...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] [Q] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] [D] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(r\"J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish-bert-botxo-qa-squad\")\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(r\"J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish-bert-botxo-qa-squad\")\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2\\bert_config.json')\n",
    "#model = AutoModelForPreTraining.from_pretrained(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2\\bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence: \n",
      " ['[CLS]', '[UNK]', 'q', '[UNK]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenize the first document: \n",
      " ['[CLS]', '[UNK]', 'd', '[UNK]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "#tokenize queries\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenize the first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenize the first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of sequences\n",
    "len_queries = [len(x) for x in tokenized_texts]\n",
    "len_documents = [len(x) for x in tokenized_docs]\n",
    "\n",
    "# Plot length of queries and documents\n",
    "n_bins = 20\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "axs[0].hist(len_queries, bins=n_bins)\n",
    "axs[0].set_title('Queries length',fontsize=12)\n",
    "axs[1].hist(len_documents, bins=n_bins)\n",
    "axs[1].set_title('Document length',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on eye-balling the plot we determine to set maximum sequence length og queries to 24 and documents to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)\n",
    "\n",
    "#param_optimizer = list(bert_base.named_parameters())\n",
    "#print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = bert_base \n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "          \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(ids, attention_mask=mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "               \n",
    "          # We apply the linear layer in line with ColBERT paper. The linear layer (which applies a linear transformation)\n",
    "          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "          # a single token in the input sequence) and outputs 32 numbers for every token\n",
    "          # so the logits are of shape (batch_size, sequence_length, 32)\n",
    "          sequence_output = self.linear1(sequence_output)\n",
    "          sequence_output = F.softmax(sequence_output, dim=1)\n",
    "\n",
    "          #linear2_output = self.linear2(linear2_output)\n",
    "\n",
    "          return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try model on first 10 query and document pairs\n",
    "sample_size = 100\n",
    "q_id    = torch.tensor(q_input_ids[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "q_mask  = torch.tensor(q_attention_masks[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "d_id    = torch.tensor(d_input_ids[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(d_attention_masks[:sample_size]).to(torch.device(device)).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24, 32])\n",
      "torch.Size([100, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "#bert_base.to(torch.device(device))\n",
    "my_model  = CustomBERTModel()\n",
    "my_model.to(torch.device(device))\n",
    "\n",
    "#BERT_base: q_outputs = bert_base(q_id, attention_mask=q_mask)\n",
    "q_outputs = my_model(q_id, mask=q_mask)\n",
    "\n",
    "\n",
    "# BERT_base: d_outputs = bert_base(d_id, attention_mask=d_mask) \n",
    "d_outputs = my_model(d_id, mask=d_mask)\n",
    "\n",
    "\n",
    "\n",
    "#With bert_base shape is: torch.Size([10, 24, 768]) and torch.Size([10, 128, 768])\n",
    "print(q_outputs.shape) \n",
    "print(d_outputs.shape) \n",
    "\n",
    "# [1] må være model output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query embedding and transposed document embedding:\n",
      "q shape:    torch.Size([24, 32])\n",
      "d^T shape:  torch.Size([32, 128])\n",
      "\n",
      "q*d^T shape: torch.Size([24, 128])\n",
      "\n",
      "max(q*d).shape torch.Size([24])\n",
      "\n",
      "sum(max(q*d)) =  tensor(0.3091, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example to convince yourself about MaxSim kan be done woth matrix multiplication\n",
    "\n",
    "# set batch number to an integer to evaluate a singe query-document pair  \n",
    "batch_no = 1\n",
    "\n",
    "# define a sample of a query embedding and a transposed document embedding\n",
    "q = q_outputs[batch_no]\n",
    "d = torch.t(d_outputs[batch_no])\n",
    "\n",
    "print('Shape of query embedding and transposed document embedding:')\n",
    "print(f'q shape:   ', q.shape )\n",
    "print(f'd^T shape: ', d.shape)\n",
    "\n",
    "print(f'\\nq*d^T shape:', torch.matmul(q,d).shape)\n",
    "\n",
    "# Find maximum value in q*d^T for each query token \n",
    "maks_qd, maks_inds = torch.max(torch.matmul(q,d), dim=1)\n",
    "print(f'\\nmax(q*d).shape', maks_qd.shape)\n",
    "\n",
    "sum_qd = torch.sum(maks_qd)\n",
    "print(f'\\nsum(max(q*d)) = ', sum_qd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_outputs.shape:                 torch.Size([100, 128, 32])\n",
      "d_outputs.permute(0,2,1).shape:  torch.Size([100, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "# Check how to Transpose document embeddings for entire batch at ones \n",
    "print('d_outputs.shape:                ', d_outputs.shape)\n",
    "print('d_outputs.permute(0,2,1).shape: ', d_outputs.permute(0,2,1).shape)\n",
    "\n",
    "# Check whether permutation is in fact a matrix transform for each batch: \n",
    "assert all(d_outputs[0][0,:] == d_outputs.permute(0,2,1)[0][:,0]), 'AssertError: matrix transpose is wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mm shape:  torch.Size([100, 24, 128])\n",
      "maks_qd.shape:   torch.Size([100, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "# Now batch multiply, i.e. tensor multiplication or rather matrix mulætiply for each batch\n",
    "batch_mm = torch.bmm(q_outputs, d_outputs.permute(0,2,1))\n",
    "print('batch_mm shape: ', batch_mm.shape)\n",
    "\n",
    "# Find maximum value in q*d^T for each query token in each batch\n",
    "maks_qd, maks_inds = torch.max(batch_mm, dim=2)\n",
    "print('maks_qd.shape:  ', maks_qd.shape)\n",
    "\n",
    "\n",
    "sum_qd = torch.sum(maks_qd, dim=1)\n",
    "sum_qd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given a query sequence $q = q_0 q_1...q_l$ and a document sequence $d = d_0 d_1...d_n$, we compute the bags of embeddings $E_q$ and $E_d$ in the following manner:\n",
    "\n",
    "* $E_q$ := Normalize( CNN( BERT(“[Q]$q_0 q_1...q_l$ ##...#”) ) )\n",
    "\n",
    "* $E_d$ := Normalize( CNN( BERT(“[D]$d_0 d_1...d_l$ ...d_n”) ) )\n",
    "\n",
    "where '#' refers to the [mask] tokens. In my implementation of ColBERT the output dimensions are as follow:\n",
    "\\begin{align*}\n",
    "    dim(E_q) = [batch_{size} \\times 24 \\times 32] \\\\\n",
    "    dim(E_d) = [batch_{size} \\times 128 \\times 32]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The relevancy score, MaxSim, is defined as follows:\n",
    "\n",
    "$$ S_{q,d} = \\sum_{i \\in ||E_q||} \\max_{j \\in ||E_d||} E_{q_i} * E_{d_j}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MaxSim(q, D):\n",
    "    '''Takes in a query, q, and return it's similarity score to\n",
    "        all documents in  D.'''\n",
    "\n",
    "    # repeat q for faster matrix multiplication (faster than loop)\n",
    "    batch_size=D.shape[0]\n",
    "    q_X = q.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # multiply the same query q against all documents (in D)\n",
    "    batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "    \n",
    "    maks, _ = torch.max(batch_mm, dim=2) # dim=1 or dim=2\n",
    "    \n",
    "    # Sum over maximum values --> return vector of length len(D)\n",
    "    S_qD = torch.sum(maks, dim=1)\n",
    "    \n",
    "    return S_qD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_doc_score = []\n",
    "most_similar_docID = []\n",
    "\n",
    " # Define D as all documents:\n",
    "D = d_outputs\n",
    "\n",
    "for q_no in tqdm(range(sample_size)):\n",
    "    \n",
    "    # Select one query\n",
    "    q = q_outputs[q_no]\n",
    "\n",
    "    # Compute similarity scores for all \n",
    "    S_qD = MaxSim(q, D)\n",
    "    maks, maks_id = torch.max(S_qD, dim=0)\n",
    "\n",
    "    most_similar_doc_score.append(float(maks))\n",
    "    most_similar_docID.append(int(maks_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "\n",
    "model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
    "model.to(torch.device(device)) ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion# TODO\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "for epoch in epochs:\n",
    "    for zip(q_batch, d_batch) in zip(q_data_loader, d_data_loader): ## If you have a DataLoader()  object to get the data.# TODO\n",
    "\n",
    "        # assign batch of query and document data\n",
    "        q_data = q_batch[0]\n",
    "        targets = q_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "         \n",
    "        d_data = d_batch[0]\n",
    "        #d_targets = d_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "        # Target is the same for query and documents. # TODO\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "\n",
    "        ##### Queries\n",
    "        #q_encoding = tokenizer.batch_encode_plus(q_data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        q_outputs = model(q_input_ids, attention_mask=q_attention_masks)\n",
    "        q_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        #q_input_ids = q_encoding['input_ids']\n",
    "        #q_attention_mask = q_encoding['attention_mask']\n",
    "        \n",
    "        ##### Documents\n",
    "        d_outputs = model(d_input_ids, attention_mask=d_attention_masks)\n",
    "        d_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        \n",
    "        maxsim = MaxSim(q_outputs, d_outputs) # vector of length batch_size\n",
    "        loss = criterion(maxsim, targets)\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(df, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_dataloader:\n",
    "#    print(batch.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

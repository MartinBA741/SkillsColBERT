{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "# specify CPU or GPU as device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r'.\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add special tokens to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer\n",
    "The BERT tokenizer is very storage efficient way of splitting a sequence into words - or rather tokens of subwords. The tokenizer uses WordPiece which uses subwords. That is splitting words into multiple words in order to keep the vocabulary smaller. That way, the vocabulary does not need to keep both: \"boy\" and \"boys\" but only \"boy\" and \"s\" where \"s\" can be used in many other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence: \n",
      " ['[CLS]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenized first document: \n",
      " ['[CLS]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "# Tokenize queries and documents\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenized first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenized first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n",
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n",
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)\n",
    "\n",
    "# Freeze parameters of BERT Base as I only will train last layer\n",
    "for param in bert_base.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in bert_base.parameters():\n",
    "#    print(param)  # param.requires_grad = False\n",
    "\n",
    "#param_optimizer = list(bert_base.named_parameters())\n",
    "#print(param_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyColBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(MyColBERT, self).__init__()\n",
    "          self.bert = bert_base \n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "          \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(ids, attention_mask=mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "               \n",
    "          # We apply the linear layer in line with ColBERT paper. The linear layer (which applies a linear transformation)\n",
    "          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "          # a single token in the input sequence) and outputs 32 numbers for every token\n",
    "          # so the logits are of shape (batch_size, sequence_length, 32)\n",
    "          sequence_output = self.linear1(sequence_output)\n",
    "          sequence_output = F.softmax(sequence_output, dim=1)\n",
    "\n",
    "          #linear2_output = self.linear2(linear2_output)\n",
    "\n",
    "          return sequence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given a query sequence $q = q_0 q_1...q_l$ and a document sequence $d = d_0 d_1...d_n$, we compute the bags of embeddings $E_q$ and $E_d$ in the following manner:\n",
    "\n",
    "* $E_q$ := Normalize( CNN( BERT(“[Q]$q_0 q_1...q_l$ ##...#”) ) )\n",
    "\n",
    "* $E_d$ := Normalize( CNN( BERT(“[D]$d_0 d_1...d_l$ ...d_n”) ) )\n",
    "\n",
    "where '#' refers to the [mask] tokens. In my implementation of ColBERT the output dimensions are as follow:\n",
    "\\begin{align*}\n",
    "    dim(E_q) = [batch_{size} \\times 24 \\times 32] \\\\\n",
    "    dim(E_d) = [batch_{size} \\times 128 \\times 32]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The relevancy score, MaxSim, is defined as follows:\n",
    "\n",
    "$$ S_{q,d} = \\sum_{i \\in ||E_q||} \\max_{j \\in ||E_d||} E_{q_i} * E_{d_j}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MaxSim(q, D):\n",
    "    '''Takes in a query, q, and return it's similarity score to\n",
    "        all documents in  D.'''\n",
    "\n",
    "    # repeat q for faster matrix multiplication (faster than loop)\n",
    "    batch_size=D.shape[0]\n",
    "    q_X = q.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # multiply the same query q against all documents (in D)\n",
    "    batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "    \n",
    "    maks, _ = torch.max(batch_mm, dim=2) # dim=1 or dim=2\n",
    "    \n",
    "    # Sum over maximum values --> return vector of length len(D)\n",
    "    S_qD = torch.sum(maks, dim=1)\n",
    "    \n",
    "    return S_qD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 831.89it/s]\n"
     ]
    }
   ],
   "source": [
    "most_similar_doc_score = []\n",
    "most_similar_docID = []\n",
    "\n",
    " # Define D as all documents:\n",
    "D = d_outputs\n",
    "\n",
    "for q_no in tqdm(range(sample_size)):\n",
    "    \n",
    "    # Select one query\n",
    "    q = q_outputs[q_no]\n",
    "\n",
    "    # Compute similarity scores for all \n",
    "    S_qD = MaxSim(q, D)\n",
    "    maks, maks_id = torch.max(S_qD, dim=0)\n",
    "\n",
    "    most_similar_doc_score.append(float(maks))\n",
    "    most_similar_docID.append(int(maks_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define criterion\n",
    "$$ L(q, d_i) = \\frac{exp(MaxSim(q, d_i))}{\\sum_{j \\in ||D||} exp(MaxSim(q, d_j))} = \\frac{exp( MaxSim(q, D)[i] )}{\\sum exp( MaxSim(q, D) )}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11072/3971404438.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxSim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxSim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#loss = criterion(output, labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "#criterion = \n",
    "loss = exp(MaxSim(q_outputs[k], D)[i]) / sum(exp(MaxSim(q_outputs[k], D)))\n",
    "#loss = criterion(output, labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model, loss function (criterion) and optimizer\n",
    "model = MyColBERT()\n",
    "model.to(torch.device(device))\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion #TODO:\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))  # alternative: optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in bert_base.parameters():\n",
    "#    print(param)  # param.requires_grad = False\n",
    "\n",
    "#param_optimizer = list(model.named_parameters())\n",
    "#print(param_optimizer[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epoch = 15  # Your code here!\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # Your code here!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # Your code here!\n",
    "\n",
    "        # Forward\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)        \n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #running_loss += loss.data[0]\n",
    "        running_loss += loss.item() \n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "    for zip(q_batch, d_batch) in zip(q_data_loader, d_data_loader): ## If you have a DataLoader()  object to get the data.# TODO\n",
    "\n",
    "        # assign batch of query and document data\n",
    "        q_data = q_batch[0]\n",
    "        targets = q_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "         \n",
    "        d_data = d_batch[0]\n",
    "        #d_targets = d_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "        # Target is the same for query and documents. # TODO\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "\n",
    "        ##### Queries\n",
    "        #q_encoding = tokenizer.batch_encode_plus(q_data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        q_outputs = model(q_input_ids, attention_mask=q_attention_masks)\n",
    "        q_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        #q_input_ids = q_encoding['input_ids']\n",
    "        #q_attention_mask = q_encoding['attention_mask']\n",
    "        \n",
    "        ##### Documents\n",
    "        d_outputs = model(d_input_ids, attention_mask=d_attention_masks)\n",
    "        d_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        \n",
    "        maxsim = MaxSim(q_outputs, d_outputs) # vector of length batch_size\n",
    "        loss = criterion(maxsim, targets)\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(df, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_dataloader:\n",
    "#    print(batch.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "# specify CPU or GPU as device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r'.\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add special tokens to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer\n",
    "The BERT tokenizer is very storage efficient way of splitting a sequence into words - or rather tokens of subwords. The tokenizer uses WordPiece which uses subwords. That is splitting words into multiple words in order to keep the vocabulary smaller. That way, the vocabulary does not need to keep both: \"boy\" and \"boys\" but only \"boy\" and \"s\" where \"s\" can be used in many other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence: \n",
      " ['[CLS]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenized first document: \n",
      " ['[CLS]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "# Tokenize queries and documents\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenized first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenized first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n",
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,  5657, 16302,  3730,     3,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n",
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting index\n",
    "index_spilt = 200#00\n",
    "\n",
    "# training data\n",
    "train_q_input_ids = q_input_ids[:index_spilt] \n",
    "train_d_input_ids = d_input_ids[:index_spilt] \n",
    "\n",
    "train_q_attention_masks = q_attention_masks[:index_spilt] \n",
    "train_d_attention_masks = d_attention_masks[:index_spilt] \n",
    "\n",
    "# create labels (all are correct)\n",
    "train_labels = torch.ones(train_q_input_ids.shape[0])\n",
    "\n",
    "\n",
    "# validation data\n",
    "val_q_input_ids = q_input_ids[index_spilt:index_spilt+101] \n",
    "val_d_input_ids = d_input_ids[index_spilt:index_spilt+101] \n",
    "\n",
    "val_q_attention_masks = q_attention_masks[index_spilt:index_spilt+101] \n",
    "val_d_attention_masks = d_attention_masks[index_spilt:index_spilt+101] \n",
    "\n",
    "val_labels = torch.ones(val_q_input_ids.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "dataset = TensorDataset(torch.tensor(train_q_input_ids), \n",
    "                        torch.tensor(train_d_input_ids), \n",
    "                        torch.tensor(train_q_attention_masks), \n",
    "                        torch.tensor(train_d_attention_masks), \n",
    "                        train_labels)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#for batch_idx, (x, y, z, a, l) in enumerate(loader):\n",
    "#    print(x.shape, y.shape, z.shape, a.shape, l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(torch.tensor(train_q_input_ids), \n",
    "                        torch.tensor(train_d_input_ids), \n",
    "                        torch.tensor(train_q_attention_masks), \n",
    "                        torch.tensor(train_d_attention_masks), \n",
    "                        train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(torch.tensor(val_q_input_ids), \n",
    "                        torch.tensor(val_d_input_ids), \n",
    "                        torch.tensor(val_q_attention_masks), \n",
    "                        torch.tensor(val_d_attention_masks), \n",
    "                        val_labels)\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT base model (Danish)\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)\n",
    "\n",
    "\n",
    "class MyColBERT(nn.Module):\n",
    "      def __init__(self):\n",
    "            super(MyColBERT, self).__init__()\n",
    "            self.bert = bert_base \n",
    "            ### New layers:\n",
    "            self.linear1 = nn.Linear(768, 32) \n",
    "\n",
    "            # Freeze parameters of BERT Base as I only will train last layer\n",
    "            for param in self.bert.parameters():\n",
    "                  param.requires_grad = False\n",
    "          \n",
    "\n",
    "      def forward(self, q_ids, q_mask, d_ids, d_mask, q_index):\n",
    "            query_embeddings= self.bert(q_ids[q_index:q_index+1], attention_mask=q_mask[q_index:q_index+1])[0] \n",
    "            query_embeddings = self.linear1(query_embeddings)\n",
    "            query_embeddings = F.normalize(query_embeddings, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "            #query_embeddings = F.softmax(query_embeddings, dim=1)\n",
    "\n",
    "            doc_embeddings = self.bert(d_ids, attention_mask=d_mask)[0]\n",
    "            doc_embeddings = self.linear1(doc_embeddings)\n",
    "            doc_embeddings = F.normalize(doc_embeddings, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "            #doc_embeddings = F.softmax(doc_embeddings, dim=1)\n",
    "\n",
    "            # Compute score for a query against all documents (in batch) \n",
    "#            score = F.softmax(self.MaxSim(query_embeddings[q_index], doc_embeddings), dim=0)\n",
    "            score = self.MaxSim(query_embeddings[q_index], doc_embeddings)\n",
    "\n",
    "            return score\n",
    "\n",
    "      \n",
    "      def MaxSim(self, q, D):\n",
    "            '''Takes in the embeddings of a query, q, and all documents' embeddings, D.\n",
    "                Return a tensor of the query's similarity scores to all documents in D.'''\n",
    "\n",
    "            # repeat q for faster matrix multiplication (faster than loop)\n",
    "            batch_size=D.shape[0]\n",
    "            q_X = q.repeat(batch_size, 1, 1)\n",
    "\n",
    "            # multiply the same query q against all documents (in D)\n",
    "            batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "\n",
    "            maks, maks_id = torch.max(batch_mm, dim=2) # should be (batch_size, 24)\n",
    "\n",
    "            # Sum over maximum values --> return vector of length len(D)\n",
    "            S_qD = torch.sum(maks, dim=1)\n",
    "\n",
    "            return S_qD\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given a query sequence $q = q_0 q_1...q_l$ and a document sequence $d = d_0 d_1...d_n$, we compute the bags of embeddings $E_q$ and $E_d$ in the following manner:\n",
    "\n",
    "* $E_q$ := Normalize( CNN( BERT(“[Q]$q_0 q_1...q_l$ ##...#”) ) )\n",
    "\n",
    "* $E_d$ := Normalize( CNN( BERT(“[D]$d_0 d_1...d_l$ ...d_n”) ) )\n",
    "\n",
    "where '#' refers to the [mask] tokens. In my implementation of ColBERT the output dimensions are as follow:\n",
    "\\begin{align*}\n",
    "    dim(E_q) = [batch_{size} \\times 24 \\times 32] \\\\\n",
    "    dim(E_d) = [batch_{size} \\times 128 \\times 32]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The relevancy score, MaxSim, is defined as follows:\n",
    "\n",
    "$$ S_{q,d} = \\sum_{i \\in ||E_q||} \\max_{j \\in ||E_d||} E_{q_i} * E_{d_j}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define criterion\n",
    "$$ L(q, d_i) = \\frac{exp(MaxSim(q, d_i))}{\\sum_{j \\in ||D||} exp(MaxSim(q, d_j))} = \\frac{exp( MaxSim(q, D)[i] )}{\\sum exp( MaxSim(q, D) )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(similarity_scores):\n",
    "    maks_score, maks_id = torch.max(similarity_scores, dim=0)\n",
    "    return torch.exp(maks_score) / torch.sum(torch.exp(similarity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model, loss function (criterion) and optimizer\n",
    "model = MyColBERT()\n",
    "model.to(torch.device(device))\n",
    "#criterion = my_softmax\n",
    "#criterion = nn.CrossEntropyLoss() ## If required define your own criterion #TODO:\n",
    "criterion = nn.MSELoss() ## If required define your own criterion #TODO:\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))  # alternative: optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -6.6696, -15.7458, -15.7305, -12.5499, -11.3765, -15.0673, -11.5651,\n",
       "        -13.3800, -10.5979, -11.4120], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "stide_len = 10 #batch_size # 100 or 10 for debugging\n",
    "\n",
    "q_index = 1\n",
    "\n",
    "\n",
    "q_id    = torch.tensor(q_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "d_id    = torch.tensor(d_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "q_mask  = torch.tensor(np.array(q_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(np.array(d_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "#%% Apply model to finde score of query q_index against stide_len ducuments\n",
    " \n",
    "scores = model(q_ids=q_id, q_mask=q_mask, d_ids=d_id, d_mask=d_mask, q_index=0)\n",
    "scores.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1091.2253, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.zeros(stide_len)\n",
    "label[q_index] = 1\n",
    "criterion(scores, target=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i, data in enumerate(df, 0):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] training loss: 0.090\n",
      "[1,     1] validation loss: 0.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B075420\\Anaconda3\\envs\\colbert\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] training loss: 0.036\n",
      "[2,     1] validation loss: 0.041\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epoch = 2 #15\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    train_running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        # get the inputs\n",
    "        q_ids, d_ids, q_masks, d_masks, labels = data\n",
    "        \n",
    "        # send to gpu (or cpu)\n",
    "        q_ids, d_ids, q_masks, d_masks = q_ids.to(torch.int64).to(device), d_ids.to(torch.int64).to(device), q_masks.to(torch.int64).to(device), d_masks.to(torch.int64).to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Forward\n",
    "        q_index = 0\n",
    "        scores = model(q_ids=q_ids, q_mask=q_masks, d_ids=d_ids, d_mask=d_masks, q_index=q_index)\n",
    "\n",
    "        # Define labels\n",
    "        labels = torch.zeros(batch_size)\n",
    "        labels[q_index] = 1        \n",
    "\n",
    "        # Compute score for all q and D in one output\n",
    "        loss = criterion(scores, target=labels)        \n",
    "        \n",
    "        ## backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        train_running_loss += loss.item() \n",
    "        train_iter.append(i)\n",
    "        train_loss.append(train_running_loss/len(data))\n",
    "\n",
    "        if i % 10 == 0:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] training loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, train_running_loss / 1000))\n",
    "            train_running_loss = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "        \n",
    "        # get the inputs\n",
    "        q_ids, d_ids, q_masks, d_masks, labels = data\n",
    "        \n",
    "        # send to gpu (or cpu)\n",
    "        q_ids, d_ids, q_masks, d_masks = q_ids.to(torch.int64).to(device), d_ids.to(torch.int64).to(device), q_masks.to(torch.int64).to(device), d_masks.to(torch.int64).to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Forward\n",
    "        q_index = 0\n",
    "        scores = model(q_ids=q_ids, q_mask=q_masks, d_ids=d_ids, d_mask=d_masks, q_index=q_index)\n",
    "\n",
    "\n",
    "        # Define labels\n",
    "        labels = torch.zeros(batch_size)\n",
    "        labels[q_index] = 1        \n",
    "\n",
    "        # Compute score for all q and D in one output\n",
    "        loss = criterion(scores, target=labels)               \n",
    "\n",
    "        ## backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        val_running_loss += loss.item() \n",
    "        valid_iter.append(i)\n",
    "        valid_loss.append(val_running_loss/len(data))\n",
    "\n",
    "        if i % 10 == 0:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] validation loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, val_running_loss / 1000))\n",
    "            val_running_loss = 0.0\n",
    "     \n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss\n",
    "#valid_loss\n",
    "valid_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (4,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1928/2189141799.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NLL'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\colbert\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2757\u001b[0m     return gca().plot(\n\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2759\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\colbert\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \"\"\"\n\u001b[0;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\colbert\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\colbert\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (4,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = np.arange(len(train_iter))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, valid_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_dataloader:\n",
    "#    print(batch.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

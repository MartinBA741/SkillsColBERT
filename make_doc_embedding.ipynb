{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make documentembeddings using SkillsColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'.\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu. Be patient...\n"
     ]
    }
   ],
   "source": [
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device==\"cuda\":\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "    print(f'Running on {device} with {n_gpu} number of GPUs')\n",
    "else:\n",
    "    print(f'Running on {device}. Be patient...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of document:\n",
      " [CLS] [D] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to documents\n",
    "documents =  [\"[CLS] [D] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "print(\"\\nExample of document:\\n\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first document: \n",
      " ['[CLS]', '[UNK]', 'd', '[UNK]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print (f'\\nTokenize the first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkillsColBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(SkillsColBERT, self).__init__()\n",
    "          self.bert = bert_base \n",
    "          ### New layers:\n",
    "          #TODO: \n",
    "          # self.finalLinear = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "          \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(ids, attention_mask=mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "               \n",
    "          # We apply the linear layer in line with ColBERT paper. The linear layer (which applies a linear transformation)\n",
    "          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "          # a single token in the input sequence) and outputs 32 numbers for every token\n",
    "          # so the logits are of shape (batch_size, sequence_length, 32)\n",
    "          \n",
    "          #TODO: \n",
    "          # sequence_output = self.finalLinear(sequence_output)\n",
    "          sequence_output = F.softmax(sequence_output, dim=1)\n",
    "\n",
    "          return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20 out of 674.25\n",
      "batch 40 out of 337.125\n",
      "batch 60 out of 224.75\n"
     ]
    }
   ],
   "source": [
    "# Choose batch_size\n",
    "stide_len = 20\n",
    "\n",
    "my_model  = SkillsColBERT()\n",
    "my_model.to(torch.device(device))\n",
    "\n",
    "\n",
    "# Initialize tensor to store output\n",
    "d_id    = torch.tensor(d_input_ids[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(d_attention_masks[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "doc_output = my_model(d_id, mask=d_mask)\n",
    "\n",
    "step = stide_len\n",
    "while step < 41:   # TODO: len(df):\n",
    "    if step % (10)==0:  # TODO: set to 100 or 500\n",
    "        print(f'batch {step} out of {len(df)/stide_len}')\n",
    "\n",
    "    d_id    = torch.tensor(d_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "    d_mask  = torch.tensor(np.array(d_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "    # Embeddings of documents\n",
    "    d_outputs = my_model(d_id, mask=d_mask)\n",
    "\n",
    "    # Append documents to doc_outputs\n",
    "    doc_output = torch.cat([doc_output, d_outputs], dim=0)\n",
    "    step += stide_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(doc_output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

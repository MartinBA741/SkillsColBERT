{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "# specify CPU or GPU as device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r'.\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add special tokens to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer\n",
    "The BERT tokenizer is very storage efficient way of splitting a sequence into words - or rather tokens of subwords. The tokenizer uses WordPiece which uses subwords. That is splitting words into multiple words in order to keep the vocabulary smaller. That way, the vocabulary does not need to keep both: \"boy\" and \"boys\" but only \"boy\" and \"s\" where \"s\" can be used in many other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence: \n",
      " ['[CLS]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenized first document: \n",
      " ['[CLS]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "# Tokenize queries and documents\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenized first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenized first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n",
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n",
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting index\n",
    "index_spilt = 200#00\n",
    "\n",
    "# training data\n",
    "train_q_input_ids = q_input_ids[:index_spilt] \n",
    "train_d_input_ids = d_input_ids[:index_spilt] \n",
    "\n",
    "train_q_attention_masks = q_attention_masks[:index_spilt] \n",
    "train_d_attention_masks = d_attention_masks[:index_spilt] \n",
    "\n",
    "# create labels (all are correct)\n",
    "train_labels = torch.ones(train_q_input_ids.shape[0])\n",
    "\n",
    "\n",
    "# validation data\n",
    "val_q_input_ids = q_input_ids[index_spilt:index_spilt+100] \n",
    "val_d_input_ids = d_input_ids[index_spilt:index_spilt+100] \n",
    "\n",
    "val_q_attention_masks = q_attention_masks[index_spilt:index_spilt+100] \n",
    "val_d_attention_masks = d_attention_masks[index_spilt:index_spilt+100] \n",
    "\n",
    "val_labels = torch.ones(val_q_input_ids.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataset = TensorDataset(torch.tensor(train_q_input_ids), \n",
    "                        torch.tensor(train_d_input_ids), \n",
    "                        torch.tensor(train_q_attention_masks), \n",
    "                        torch.tensor(train_d_attention_masks), \n",
    "                        train_labels)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#for batch_idx, (x, y, z, a, l) in enumerate(loader):\n",
    "#    print(x.shape, y.shape, z.shape, a.shape, l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(torch.tensor(train_q_input_ids), \n",
    "                        torch.tensor(train_d_input_ids), \n",
    "                        torch.tensor(train_q_attention_masks), \n",
    "                        torch.tensor(train_d_attention_masks), \n",
    "                        train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(torch.tensor(val_q_input_ids), \n",
    "                        torch.tensor(val_d_input_ids), \n",
    "                        torch.tensor(val_q_attention_masks), \n",
    "                        torch.tensor(val_d_attention_masks), \n",
    "                        val_labels)\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT base model (Danish)\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)\n",
    "\n",
    "\n",
    "class MyColBERT(nn.Module):\n",
    "      def __init__(self):\n",
    "            super(MyColBERT, self).__init__()\n",
    "            self.bert = bert_base \n",
    "            ### New layers:\n",
    "            self.linear1 = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "\n",
    "            # Freeze parameters of BERT Base as I only will train last layer\n",
    "            for param in self.bert.parameters():\n",
    "                  param.requires_grad = False\n",
    "          \n",
    "\n",
    "      def forward(self, q_ids, q_mask, d_ids, d_mask):\n",
    "            query_embeddings, pooled_output = self.bert(q_ids, attention_mask=q_mask) \n",
    "            query_embeddings = self.linear1(query_embeddings)\n",
    "            query_embeddings = F.softmax(query_embeddings, dim=1)\n",
    "\n",
    "            \n",
    "            doc_embeddings, pooled_output = self.bert(d_ids, attention_mask=d_mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "            doc_embeddings = self.linear1(doc_embeddings)\n",
    "            doc_embeddings = F.softmax(doc_embeddings, dim=1)\n",
    "\n",
    "            # Compute score for all queries against all documents (in batch) \n",
    "            score = torch.zeros(query_embeddings.shape[0], doc_embeddings.shape[0])\n",
    "            for i in range(0, query_embeddings.shape[0]):\n",
    "                  score[i] = F.softmax(self.MaxSim(query_embeddings[i], doc_embeddings), dim=0)\n",
    "\n",
    "            return score\n",
    "\n",
    "      \n",
    "      def MaxSim(self, q, D):\n",
    "            '''Takes in the embeddings of a query, q, and all documents' embeddings, D.\n",
    "                Return a tensor of the query's similarity scores to all documents in D.'''\n",
    "\n",
    "            # repeat q for faster matrix multiplication (faster than loop)\n",
    "            batch_size=D.shape[0]\n",
    "            q_X = q.repeat(batch_size, 1, 1)\n",
    "\n",
    "            # multiply the same query q against all documents (in D)\n",
    "            batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "\n",
    "            maks, maks_id = torch.max(batch_mm, dim=2) # should be (batch_size, 24)\n",
    "\n",
    "            # Sum over maximum values --> return vector of length len(D)\n",
    "            S_qD = torch.sum(maks, dim=1)\n",
    "\n",
    "            # Most similar document number \n",
    "            #S_qD_max, most_similar_doc_id = torch.max(S_qD, dim=0)\n",
    "\n",
    "            return S_qD\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxSim(q, D):\n",
    "            '''Takes in the embeddings of a query, q, and all documents' embeddings, D.\n",
    "                Return a tensor of the query's similarity scores to all documents in D.'''\n",
    "\n",
    "            # repeat q for faster matrix multiplication (faster than loop)\n",
    "            batch_size=D.shape[0]\n",
    "            q_X = q.repeat(batch_size, 1, 1)\n",
    "\n",
    "            # multiply the same query q against all documents (in D)\n",
    "            batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "\n",
    "            maks, maks_id = torch.max(batch_mm, dim=2) # should be (batch_size, 24)\n",
    "\n",
    "            # Sum over maximum values --> return vector of length len(D)\n",
    "            S_qD = torch.sum(maks, dim=1)\n",
    "\n",
    "            # Most similar document number \n",
    "            #S_qD_max, most_similar_doc_id = torch.max(S_qD, dim=0)\n",
    "\n",
    "            return S_qD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings, pooled_output = bert_base(q_ids, attention_mask=q_mask) \n",
    "query_embeddings = nn.Linear(768, 32)(query_embeddings)\n",
    "query_embeddings = F.softmax(query_embeddings, dim=1)\n",
    "\n",
    "doc_embeddings, pooled_output = bert_base(d_ids, attention_mask=d_mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "doc_embeddings = nn.Linear(768, 32)(doc_embeddings)\n",
    "doc_embeddings = F.softmax(doc_embeddings, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0671, 0.0671, 0.0672, 0.0671, 0.0670, 0.0670, 0.0671, 0.0671, 0.0671,\n",
       "        0.0672, 0.0671, 0.0672, 0.0670, 0.0672, 0.0671])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given a query sequence $q = q_0 q_1...q_l$ and a document sequence $d = d_0 d_1...d_n$, we compute the bags of embeddings $E_q$ and $E_d$ in the following manner:\n",
    "\n",
    "* $E_q$ := Normalize( CNN( BERT(“[Q]$q_0 q_1...q_l$ ##...#”) ) )\n",
    "\n",
    "* $E_d$ := Normalize( CNN( BERT(“[D]$d_0 d_1...d_l$ ...d_n”) ) )\n",
    "\n",
    "where '#' refers to the [mask] tokens. In my implementation of ColBERT the output dimensions are as follow:\n",
    "\\begin{align*}\n",
    "    dim(E_q) = [batch_{size} \\times 24 \\times 32] \\\\\n",
    "    dim(E_d) = [batch_{size} \\times 128 \\times 32]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The relevancy score, MaxSim, is defined as follows:\n",
    "\n",
    "$$ S_{q,d} = \\sum_{i \\in ||E_q||} \\max_{j \\in ||E_d||} E_{q_i} * E_{d_j}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define criterion\n",
    "$$ L(q, d_i) = \\frac{exp(MaxSim(q, d_i))}{\\sum_{j \\in ||D||} exp(MaxSim(q, d_j))} = \\frac{exp( MaxSim(q, D)[i] )}{\\sum exp( MaxSim(q, D) )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(similarity_scores):\n",
    "    maks_score, maks_id = torch.max(similarity_scores, dim=1)\n",
    "    return torch.exp(maks_score) / torch.sum(torch.exp(similarity_scores), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def old_my_softmax(similarity_scores):\n",
    "#    maks_score, maks_id = torch.max(similarity_scores, dim=0)\n",
    "#    return torch.exp(maks_score) / torch.sum(torch.exp(similarity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model, loss function (criterion) and optimizer\n",
    "model = MyColBERT()\n",
    "model.to(torch.device(device))\n",
    "criterion = my_softmax\n",
    "#criterion = nn.CrossEntropyLoss() ## If required define your own criterion #TODO:\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))  # alternative: optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0673, 0.0672, 0.0664, 0.0668, 0.0669, 0.0677, 0.0662, 0.0671, 0.0662,\n",
       "         0.0666, 0.0666, 0.0664, 0.0663, 0.0659, 0.0664],\n",
       "        [0.0672, 0.0671, 0.0665, 0.0667, 0.0669, 0.0677, 0.0662, 0.0671, 0.0662,\n",
       "         0.0667, 0.0666, 0.0665, 0.0662, 0.0660, 0.0662],\n",
       "        [0.0673, 0.0672, 0.0665, 0.0667, 0.0669, 0.0677, 0.0661, 0.0671, 0.0662,\n",
       "         0.0668, 0.0666, 0.0664, 0.0663, 0.0659, 0.0662],\n",
       "        [0.0672, 0.0671, 0.0664, 0.0667, 0.0669, 0.0677, 0.0663, 0.0671, 0.0663,\n",
       "         0.0668, 0.0667, 0.0664, 0.0661, 0.0660, 0.0664],\n",
       "        [0.0673, 0.0672, 0.0664, 0.0669, 0.0669, 0.0678, 0.0663, 0.0671, 0.0662,\n",
       "         0.0666, 0.0666, 0.0663, 0.0662, 0.0659, 0.0663],\n",
       "        [0.0673, 0.0672, 0.0665, 0.0668, 0.0669, 0.0677, 0.0662, 0.0672, 0.0662,\n",
       "         0.0667, 0.0666, 0.0663, 0.0662, 0.0660, 0.0663],\n",
       "        [0.0673, 0.0671, 0.0665, 0.0668, 0.0669, 0.0677, 0.0661, 0.0671, 0.0662,\n",
       "         0.0668, 0.0666, 0.0664, 0.0662, 0.0661, 0.0663],\n",
       "        [0.0673, 0.0670, 0.0665, 0.0667, 0.0668, 0.0678, 0.0662, 0.0670, 0.0662,\n",
       "         0.0668, 0.0666, 0.0665, 0.0663, 0.0659, 0.0664],\n",
       "        [0.0674, 0.0672, 0.0664, 0.0668, 0.0669, 0.0678, 0.0661, 0.0671, 0.0661,\n",
       "         0.0667, 0.0666, 0.0664, 0.0663, 0.0659, 0.0663],\n",
       "        [0.0672, 0.0670, 0.0665, 0.0667, 0.0670, 0.0677, 0.0662, 0.0672, 0.0662,\n",
       "         0.0668, 0.0666, 0.0664, 0.0663, 0.0659, 0.0663],\n",
       "        [0.0672, 0.0671, 0.0664, 0.0668, 0.0669, 0.0678, 0.0661, 0.0671, 0.0663,\n",
       "         0.0668, 0.0667, 0.0664, 0.0662, 0.0659, 0.0663],\n",
       "        [0.0673, 0.0671, 0.0665, 0.0668, 0.0669, 0.0677, 0.0661, 0.0672, 0.0662,\n",
       "         0.0667, 0.0665, 0.0664, 0.0663, 0.0658, 0.0664],\n",
       "        [0.0672, 0.0671, 0.0664, 0.0667, 0.0669, 0.0677, 0.0662, 0.0670, 0.0664,\n",
       "         0.0667, 0.0667, 0.0664, 0.0663, 0.0660, 0.0662],\n",
       "        [0.0672, 0.0672, 0.0665, 0.0667, 0.0669, 0.0677, 0.0662, 0.0672, 0.0662,\n",
       "         0.0667, 0.0666, 0.0663, 0.0662, 0.0659, 0.0663],\n",
       "        [0.0673, 0.0672, 0.0664, 0.0667, 0.0670, 0.0678, 0.0662, 0.0671, 0.0663,\n",
       "         0.0667, 0.0666, 0.0664, 0.0662, 0.0659, 0.0662]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "stide_len = 15 #batch_size # 100 or 10 for debugging\n",
    "\n",
    "q_index = 0\n",
    "\n",
    "\n",
    "q_id    = torch.tensor(q_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "d_id    = torch.tensor(d_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "q_mask  = torch.tensor(np.array(q_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(np.array(d_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "#%% Apply model to finde score of query q_index against stide_len ducuments\n",
    " \n",
    "scores = model(q_ids=q_id, q_mask=q_mask, d_ids=d_id, d_mask=d_mask)\n",
    "scores.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 15])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3900/3298085941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Compute score for all q and D in one output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m## backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3900/3810628132.py\u001b[0m in \u001b[0;36mmy_softmax\u001b[1;34m(similarity_scores)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmaks_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaks_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaks_score\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epoch = 1 #15\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    train_running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        # get the inputs\n",
    "        q_ids, d_ids, q_masks, d_masks, labels = data\n",
    "        \n",
    "        # send to gpu (or cpu)\n",
    "        q_ids, d_ids, q_masks, d_masks = q_ids.to(torch.int64).to(device), d_ids.to(torch.int64).to(device), q_masks.to(torch.int64).to(device), d_masks.to(torch.int64).to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Forward\n",
    "        scores = model(q_ids=q_id, q_mask=q_mask, d_ids=d_id, d_mask=d_mask)\n",
    "\n",
    "        for j in range(scores.shape[0]): # Compute score for all q and D in one output\n",
    "            loss = criterion(scores[j])        \n",
    "        \n",
    "            ## backward\n",
    "            loss.backward()\n",
    "        \n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "        \n",
    "            # print statistics\n",
    "            train_running_loss += loss.item() \n",
    "            train_iter.append(i+j)\n",
    "\n",
    "        if i % 10 == 0:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, train_running_loss / 1000))\n",
    "            train_running_loss = 0.0\n",
    "\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "        \n",
    "        # get the inputs\n",
    "        q_ids, d_ids, q_masks, d_masks, labels = data\n",
    "        \n",
    "        # send to gpu (or cpu)\n",
    "        q_ids, d_ids, q_masks, d_masks = q_ids.to(torch.int64).to(device), d_ids.to(torch.int64).to(device), q_masks.to(torch.int64).to(device), d_masks.to(torch.int64).to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Forward\n",
    "        scores = model(q_ids=q_id, q_mask=q_mask, d_ids=d_id, d_mask=d_mask)\n",
    "    \n",
    "        for j in range(scores.shape[0]):\n",
    "            # Compute score for all q and D in one output\n",
    "            loss = criterion(scores[j])        \n",
    "\n",
    "            ## backward\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            val_running_loss += loss.item() \n",
    "            valid_iter.append(i+j)\n",
    "\n",
    "        if i % 10 == 0:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, val_running_loss / 1000))\n",
    "            val_running_loss = 0.0\n",
    "            \n",
    "        # PLOT progress\n",
    "        #if i % 10 == 0:\n",
    "        #    fig = plt.figure(figsize=(12,4))\n",
    "        #    plt.subplot(1, 2, 1)\n",
    "        #    plt.plot(train_iter, train_running_loss, label='train_loss')\n",
    "        #    #plt.plot(valid_iter, val_running_loss, label='valid_loss')\n",
    "        #    plt.legend()\n",
    "#\n",
    "            #plt.subplot(1, 2, 2)\n",
    "            #plt.plot(train_iter, train_accs, label='train_accs')\n",
    "            #plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "            #plt.legend()\n",
    "            #plt.show()\n",
    "            #clear_output(wait=True)\n",
    "        #print(\"Train, it: {} loss: {:.2f} \".format(i, train_running_loss[-1]))#, train_accs[-1]))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_dataloader:\n",
    "#    print(batch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "#df['target'] = 1  # set target = 1, i.e. correct match\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Assign wrong documents to queries by spliting df in 2 halfs\n",
    "#split_index = round(len(df)/2) \n",
    "#df_fail = pd.DataFrame()\n",
    "#df_fail['query'] = df['query'].copy()\n",
    "#df_fail['documents'] = str()\n",
    "#df_fail['documents'].iloc[:split_index] = df['documents'].iloc[split_index+1:].copy()\n",
    "#df_fail['documents'].iloc[split_index:] = df['documents'].iloc[:split_index+1].copy()\n",
    "#df_fail['target'] = 0 # set target = 0, i.e. not a correct match\n",
    "\n",
    "## Append dataframes \n",
    "#df = df.append(df_fail, ignore_index=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu. Be patient...\n"
     ]
    }
   ],
   "source": [
    "# specify GPU device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(r\"J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish-bert-botxo-qa-squad\")\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(r\"J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish-bert-botxo-qa-squad\")\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2\\bert_config.json')\n",
    "#model = AutoModelForPreTraining.from_pretrained(r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2\\bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence: \n",
      " ['[CLS]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenize the first document: \n",
      " ['[CLS]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "#tokenize queries\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenize the first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenize the first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of sequences\n",
    "len_queries = [len(x) for x in tokenized_texts]\n",
    "len_documents = [len(x) for x in tokenized_docs]\n",
    "\n",
    "# Plot length of queries and documents\n",
    "n_bins = 20\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "axs[0].hist(len_queries, bins=n_bins)\n",
    "axs[0].set_title('Queries length',fontsize=12)\n",
    "axs[1].hist(len_documents, bins=n_bins)\n",
    "axs[1].set_title('Document length',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on eye-balling the plot we determine to set maximum sequence length og queries to 24 and documents to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)\n",
    "\n",
    "#param_optimizer = list(bert_base.named_parameters())\n",
    "#print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = bert_base \n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "          \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(ids, attention_mask=mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "               \n",
    "          # We apply the linear layer in line with ColBERT paper. The linear layer (which applies a linear transformation)\n",
    "          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "          # a single token in the input sequence) and outputs 32 numbers for every token\n",
    "          # so the logits are of shape (batch_size, sequence_length, 32)\n",
    "          sequence_output = self.linear1(sequence_output)\n",
    "          sequence_output = F.softmax(sequence_output, dim=1)\n",
    "\n",
    "          #linear2_output = self.linear2(linear2_output)\n",
    "\n",
    "          return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try model on first 10 query and document pairs\n",
    "sample_size = 10\n",
    "q_id    = torch.tensor(q_input_ids[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "q_mask  = torch.tensor(q_attention_masks[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "d_id    = torch.tensor(d_input_ids[:sample_size]).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(d_attention_masks[:sample_size]).to(torch.device(device)).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 24, 32])\n",
      "torch.Size([10, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "#bert_base.to(torch.device(device))\n",
    "my_model  = CustomBERTModel()\n",
    "my_model.to(torch.device(device))\n",
    "\n",
    "#BERT_base: q_outputs = bert_base(q_id, attention_mask=q_mask)\n",
    "q_outputs = my_model(q_id, mask=q_mask)\n",
    "\n",
    "\n",
    "# BERT_base: d_outputs = bert_base(d_id, attention_mask=d_mask) \n",
    "d_outputs = my_model(d_id, mask=d_mask)\n",
    "\n",
    "\n",
    "\n",
    "#With bert_base shape is: torch.Size([10, 24, 768]) and torch.Size([10, 128, 768])\n",
    "print(q_outputs.shape) \n",
    "print(d_outputs.shape) \n",
    "\n",
    "# [1] må være model output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query embedding and transposed document embedding:\n",
      "q shape:    torch.Size([24, 32])\n",
      "d^T shape:  torch.Size([32, 128])\n",
      "\n",
      "q*d^T shape: torch.Size([24, 128])\n",
      "\n",
      "max(q*d).shape torch.Size([24])\n",
      "\n",
      "sum(max(q*d)) =  tensor(0.2964, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example to convince yourself about MaxSim kan be done woth matrix multiplication\n",
    "\n",
    "# set batch number to an integer to evaluate a singe query-document pair  \n",
    "batch_no = 1\n",
    "\n",
    "# define a sample of a query embedding and a transposed document embedding\n",
    "q = q_outputs[batch_no]\n",
    "d = torch.t(d_outputs[batch_no])\n",
    "\n",
    "print('Shape of query embedding and transposed document embedding:')\n",
    "print(f'q shape:   ', q.shape )\n",
    "print(f'd^T shape: ', d.shape)\n",
    "\n",
    "print(f'\\nq*d^T shape:', torch.matmul(q,d).shape)\n",
    "\n",
    "# Find maximum value in q*d^T for each query token \n",
    "maks_qd, maks_inds = torch.max(torch.matmul(q,d), dim=1)\n",
    "print(f'\\nmax(q*d).shape', maks_qd.shape)\n",
    "\n",
    "sum_qd = torch.sum(maks_qd)\n",
    "print(f'\\nsum(max(q*d)) = ', sum_qd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_outputs.shape:                 torch.Size([10, 128, 32])\n",
      "d_outputs.permute(0,2,1).shape:  torch.Size([10, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "# Check how to Transpose document embeddings for entire batch at ones \n",
    "print('d_outputs.shape:                ', d_outputs.shape)\n",
    "print('d_outputs.permute(0,2,1).shape: ', d_outputs.permute(0,2,1).shape)\n",
    "\n",
    "# Check whether permutation is in fact a matrix transform for each batch: \n",
    "assert all(d_outputs[0][0,:] == d_outputs.permute(0,2,1)[0][:,0]), 'AssertError: matrix transpose is wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mm shape:  torch.Size([10, 24, 128])\n",
      "maks_qd.shape:   torch.Size([10, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "# Now batch multiply, i.e. tensor multiplication or rather matrix mulætiply for each batch\n",
    "batch_mm = torch.bmm(q_outputs, d_outputs.permute(0,2,1))\n",
    "print('batch_mm shape: ', batch_mm.shape)\n",
    "\n",
    "# Find maximum value in q*d^T for each query token in each batch\n",
    "maks_qd, maks_inds = torch.max(batch_mm, dim=2)\n",
    "print('maks_qd.shape:  ', maks_qd.shape)\n",
    "\n",
    "\n",
    "sum_qd = torch.sum(maks_qd, dim=1)\n",
    "sum_qd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given a query sequence $q = q_0 q_1...q_l$ and a document sequence $d = d_0 d_1...d_n$, we compute the bags of embeddings $E_q$ and $E_d$ in the following manner:\n",
    "\n",
    "* $E_q$ := Normalize( CNN( BERT(“[Q]$q_0 q_1...q_l$ ##...#”) ) )\n",
    "\n",
    "* $E_d$ := Normalize( CNN( BERT(“[D]$d_0 d_1...d_l$ ...d_n”) ) )\n",
    "\n",
    "where '#' refers to the [mask] tokens. In my implementation of ColBERT the output dimensions are as follow:\n",
    "\\begin{align*}\n",
    "    dim(E_q) = [batch_{size} \\times 24 \\times 32] \\\\\n",
    "    dim(E_d) = [batch_{size} \\times 128 \\times 32]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The relevancy score, MaxSim, is defined as follows:\n",
    "\n",
    "$$ S_{q,d} = \\sum_{i \\in ||E_q||} \\max_{j \\in ||E_d||} E_{q_i} * E_{d_j}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MaxSim(q, D):\n",
    "    '''Takes in a query, q, and return it's similarity score to\n",
    "        all documents in  D.'''\n",
    "\n",
    "    # repeat q for faster matrix multiplication (faster than loop)\n",
    "    batch_size=D.shape[0]\n",
    "    q_X = q.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # multiply the same query q against all documents (in D)\n",
    "    batch_mm = torch.bmm(q_X, D.permute(0,2,1))\n",
    "    \n",
    "    maks, _ = torch.max(batch_mm, dim=2) # dim=1 or dim=2\n",
    "    \n",
    "    # Sum over maximum values --> return vector of length len(D)\n",
    "    S_qD = torch.sum(maks, dim=1)\n",
    "    \n",
    "    return S_qD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 831.89it/s]\n"
     ]
    }
   ],
   "source": [
    "most_similar_doc_score = []\n",
    "most_similar_docID = []\n",
    "\n",
    " # Define D as all documents:\n",
    "D = d_outputs\n",
    "\n",
    "for q_no in tqdm(range(sample_size)):\n",
    "    \n",
    "    # Select one query\n",
    "    q = q_outputs[q_no]\n",
    "\n",
    "    # Compute similarity scores for all \n",
    "    S_qD = MaxSim(q, D)\n",
    "    maks, maks_id = torch.max(S_qD, dim=0)\n",
    "\n",
    "    most_similar_doc_score.append(float(maks))\n",
    "    most_similar_docID.append(int(maks_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "\n",
    "model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
    "model.to(torch.device(device)) ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion# TODO\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "for epoch in epochs:\n",
    "    for zip(q_batch, d_batch) in zip(q_data_loader, d_data_loader): ## If you have a DataLoader()  object to get the data.# TODO\n",
    "\n",
    "        # assign batch of query and document data\n",
    "        q_data = q_batch[0]\n",
    "        targets = q_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "         \n",
    "        d_data = d_batch[0]\n",
    "        #d_targets = d_batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "        # Target is the same for query and documents. # TODO\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "\n",
    "        ##### Queries\n",
    "        #q_encoding = tokenizer.batch_encode_plus(q_data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        q_outputs = model(q_input_ids, attention_mask=q_attention_masks)\n",
    "        q_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        #q_input_ids = q_encoding['input_ids']\n",
    "        #q_attention_mask = q_encoding['attention_mask']\n",
    "        \n",
    "        ##### Documents\n",
    "        d_outputs = model(d_input_ids, attention_mask=d_attention_masks)\n",
    "        d_outputs = F.log_softmax(q_outputs, dim=1)\n",
    "        \n",
    "        maxsim = MaxSim(q_outputs, d_outputs) # vector of length batch_size\n",
    "        loss = criterion(maxsim, targets)\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(df, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_dataloader:\n",
    "#    print(batch.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

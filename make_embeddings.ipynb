{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make query and document embeddings using (Skills-/Col-) BERT\n",
    "This notebook will preprocess data and tokenize queries and documents before sending it all through a BERT model to obtain embeddings. The embeddings of quries and documents are stored locally in order to load them for training a new final linear layer on top of BERT and later to compute similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# !pip install pytorch-pretrained-bert pytorch-nlp keras scikit-learn matplotlib tensorflow\n",
    "\n",
    "#https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lede musikalsk personale</td>\n",
       "      <td>Tildele og forvalte personaleopgaver på område...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>føre tilsyn med fængselsprocedurer</td>\n",
       "      <td>Føre tilsyn med driften af et fængsel eller an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvende antioppressiv praksis</td>\n",
       "      <td>Identificere undertrykkelse i samfund, økonomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kontrollere overensstemmelse med jernbaneforsk...</td>\n",
       "      <td>Inspicere rullende materiel, komponenter og sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identificere tilgængelige tjenester</td>\n",
       "      <td>Identificere de forskellige tjenester, der er ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                           lede musikalsk personale   \n",
       "1                 føre tilsyn med fængselsprocedurer   \n",
       "2                      anvende antioppressiv praksis   \n",
       "3  kontrollere overensstemmelse med jernbaneforsk...   \n",
       "4                identificere tilgængelige tjenester   \n",
       "\n",
       "                                           documents  \n",
       "0  Tildele og forvalte personaleopgaver på område...  \n",
       "1  Føre tilsyn med driften af et fængsel eller an...  \n",
       "2  Identificere undertrykkelse i samfund, økonomi...  \n",
       "3  Inspicere rullende materiel, komponenter og sy...  \n",
       "4  Identificere de forskellige tjenester, der er ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'.\\data\\skills_description.csv', sep='\\t', encoding='utf-8')\n",
    "df = df.rename(columns={'preferredLabel':'query', 'description': 'documents'})\n",
    "df = df[['query', 'documents']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu. Be patient...\n"
     ]
    }
   ],
   "source": [
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device==\"cuda\":\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "    print(f'Running on {device} with {n_gpu} number of GPUs')\n",
    "else:\n",
    "    print(f'Running on {device}. Be patient...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add special tokens to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of query:\n",
      " [CLS] lede musikalsk personale [SEP]\n",
      "\n",
      "Example of document:\n",
      " [CLS] Tildele og forvalte personaleopgaver på områder såsom instrumentering, bearbejdning, reproduktion af musik og stemmetræning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# add special ColBERT tokens to queries and documents\n",
    "queries = [\"[CLS] \" + query + \" [SEP]\" for query in df['query']]\n",
    "documents =  [\"[CLS] \" + query + \" [SEP]\" for query in df['documents']]\n",
    "print(\"Example of query:\\n\", queries[0])\n",
    "print(\"\\nExample of document:\\n\", documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer\n",
    "The BERT tokenizer is very storage efficient way of splitting a sequence into words - or rather tokens of subwords. The tokenizer uses WordPiece which uses subwords. That is splitting words into multiple words in order to keep the vocabulary smaller. That way, the vocabulary does not need to keep both: \"boy\" and \"boys\" but only \"boy\" and \"s\" where \"s\" can be used in many other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence: \n",
      " ['[CLS]', 'lede', 'musikalsk', 'personale', '[SEP]']\n",
      "\n",
      "Tokenized first document: \n",
      " ['[CLS]', 'tildele', 'og', 'forvalt', '##e', 'personale', '##opgaver', 'pa', 'om', '##rad', '##er', 'sas', '##om', 'instrumenter', '##ing', ',', 'bearbejdning', ',', 'reproduktion', 'af', 'musik', 'og', 'stemme', '##træning', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "model_path = r'J:\\VOA\\MABI\\Deep Learning\\my_DTU_project\\Models\\danish_bert_uncased_v2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "\n",
    "# Tokenize queries and documents\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in queries]\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "print(f'Tokenized first sentence: \\n {tokenized_texts[0]}')\n",
    "print (f'\\nTokenized first document: \\n {tokenized_docs[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens to ids \n",
    "- Now assign ids to the tokens that the respective subword in the vocabulary (see seperate vocab file associated with the model).\n",
    "- Also pad input sequences to predetermined length (24 and 128). This hypwerparameter is determined by plotting sequence length earlier.\n",
    "- Keep track of what is padded and masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of query ids:\n",
      " q_input_ids.shape = (13485, 24)\n",
      "Shape of query attention mask:\n",
      " q_attention_masks = (13485, 24)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum query length. \n",
    "MAX_LEN_Q = 24\n",
    "\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "q_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "q_input_ids = pad_sequences(q_input_ids, maxlen=MAX_LEN_Q, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of query ids:\\n q_input_ids.shape = {q_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create query attention masks\n",
    "q_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in q_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  q_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of query attention mask:\\n q_attention_masks = {np.shape(q_attention_masks)}')\n",
    "\n",
    "assert q_input_ids.shape == np.shape(q_attention_masks), 'dimensions of q_input_ids and q_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids.shape: (13485, 128)\n",
      "Shape of d_attention_masks: (13485, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum document length. \n",
    "MAX_LEN_DOC = 128\n",
    "# Pad our input tokens\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_docs]\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN_DOC, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(f'Shape of input_ids.shape: {d_input_ids.shape}')\n",
    "\n",
    "\n",
    "# Create attention masks for documents\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "\n",
    "print(f'Shape of d_attention_masks: {np.shape(d_attention_masks)}')\n",
    "\n",
    "assert d_input_ids.shape == np.shape(d_attention_masks), 'dimensions of document d_input_ids and d_attention_mask do not match' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model\n",
    "Queries and documents have now been tokenized to the vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path + r'\\bert_config.json')\n",
    "bert_base = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkillsColBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(SkillsColBERT, self).__init__()\n",
    "          self.bert = bert_base \n",
    "          ### New layers:\n",
    "          #TODO: \n",
    "          # self.finalLinear = nn.Linear(768, 32) # 32 is \"low\" for faster computation of MaxSim (it is independent of sequence lentgh)\n",
    "          \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(ids, attention_mask=mask) # sequence_output shape is: (batch_size, sequence_length, 768)\n",
    "               \n",
    "          # We apply the linear layer in line with ColBERT paper. The linear layer (which applies a linear transformation)\n",
    "          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "          # a single token in the input sequence) and outputs 32 numbers for every token\n",
    "          # so the logits are of shape (batch_size, sequence_length, 32)\n",
    "          \n",
    "          #TODO: \n",
    "          # sequence_output = self.finalLinear(sequence_output)\n",
    "          sequence_output = F.softmax(sequence_output, dim=1)\n",
    "\n",
    "          return sequence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings for queries and documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process documents through BERT model to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of size 100 out of 134.85\n",
      "batch 1 of size 100 out of 134.85\n",
      "batch 2 of size 100 out of 134.85\n",
      "batch 3 of size 100 out of 134.85\n",
      "batch 4 of size 100 out of 134.85\n",
      "batch 5 of size 100 out of 134.85\n",
      "batch 6 of size 100 out of 134.85\n",
      "batch 7 of size 100 out of 134.85\n",
      "batch 8 of size 100 out of 134.85\n",
      "batch 9 of size 100 out of 134.85\n",
      "batch 10 of size 100 out of 134.85\n",
      "batch 11 of size 100 out of 134.85\n",
      "batch 12 of size 100 out of 134.85\n",
      "batch 13 of size 100 out of 134.85\n",
      "batch 14 of size 100 out of 134.85\n",
      "batch 15 of size 100 out of 134.85\n",
      "batch 16 of size 100 out of 134.85\n",
      "batch 17 of size 100 out of 134.85\n",
      "batch 18 of size 100 out of 134.85\n",
      "batch 19 of size 100 out of 134.85\n",
      "batch 20 of size 100 out of 134.85\n",
      "batch 21 of size 100 out of 134.85\n",
      "batch 22 of size 100 out of 134.85\n",
      "batch 23 of size 100 out of 134.85\n",
      "batch 24 of size 100 out of 134.85\n",
      "batch 25 of size 100 out of 134.85\n",
      "batch 26 of size 100 out of 134.85\n",
      "batch 27 of size 100 out of 134.85\n",
      "batch 28 of size 100 out of 134.85\n",
      "batch 29 of size 100 out of 134.85\n",
      "batch 30 of size 100 out of 134.85\n",
      "batch 31 of size 100 out of 134.85\n",
      "batch 32 of size 100 out of 134.85\n",
      "batch 33 of size 100 out of 134.85\n",
      "batch 34 of size 100 out of 134.85\n",
      "batch 35 of size 100 out of 134.85\n",
      "batch 36 of size 100 out of 134.85\n",
      "batch 37 of size 100 out of 134.85\n",
      "batch 38 of size 100 out of 134.85\n",
      "batch 39 of size 100 out of 134.85\n",
      "batch 40 of size 100 out of 134.85\n",
      "batch 41 of size 100 out of 134.85\n",
      "batch 42 of size 100 out of 134.85\n",
      "batch 43 of size 100 out of 134.85\n",
      "batch 44 of size 100 out of 134.85\n",
      "batch 45 of size 100 out of 134.85\n",
      "batch 46 of size 100 out of 134.85\n",
      "batch 47 of size 100 out of 134.85\n",
      "batch 48 of size 100 out of 134.85\n",
      "batch 49 of size 100 out of 134.85\n",
      "batch 50 of size 100 out of 134.85\n",
      "batch 51 of size 100 out of 134.85\n",
      "batch 52 of size 100 out of 134.85\n",
      "batch 53 of size 100 out of 134.85\n",
      "batch 54 of size 100 out of 134.85\n",
      "batch 55 of size 100 out of 134.85\n",
      "batch 56 of size 100 out of 134.85\n",
      "batch 57 of size 100 out of 134.85\n",
      "batch 58 of size 100 out of 134.85\n",
      "batch 59 of size 100 out of 134.85\n",
      "batch 60 of size 100 out of 134.85\n",
      "batch 61 of size 100 out of 134.85\n",
      "batch 62 of size 100 out of 134.85\n",
      "batch 63 of size 100 out of 134.85\n",
      "batch 64 of size 100 out of 134.85\n",
      "batch 65 of size 100 out of 134.85\n",
      "batch 66 of size 100 out of 134.85\n",
      "batch 67 of size 100 out of 134.85\n",
      "batch 68 of size 100 out of 134.85\n",
      "batch 69 of size 100 out of 134.85\n",
      "batch 70 of size 100 out of 134.85\n",
      "batch 71 of size 100 out of 134.85\n",
      "batch 72 of size 100 out of 134.85\n",
      "batch 73 of size 100 out of 134.85\n",
      "batch 74 of size 100 out of 134.85\n",
      "batch 75 of size 100 out of 134.85\n",
      "batch 76 of size 100 out of 134.85\n",
      "batch 77 of size 100 out of 134.85\n",
      "batch 78 of size 100 out of 134.85\n",
      "batch 79 of size 100 out of 134.85\n",
      "batch 80 of size 100 out of 134.85\n",
      "batch 81 of size 100 out of 134.85\n",
      "batch 82 of size 100 out of 134.85\n",
      "batch 83 of size 100 out of 134.85\n",
      "batch 84 of size 100 out of 134.85\n",
      "batch 85 of size 100 out of 134.85\n",
      "batch 86 of size 100 out of 134.85\n",
      "batch 87 of size 100 out of 134.85\n",
      "batch 88 of size 100 out of 134.85\n",
      "batch 89 of size 100 out of 134.85\n",
      "batch 90 of size 100 out of 134.85\n",
      "batch 91 of size 100 out of 134.85\n",
      "batch 92 of size 100 out of 134.85\n",
      "batch 93 of size 100 out of 134.85\n",
      "batch 94 of size 100 out of 134.85\n",
      "batch 95 of size 100 out of 134.85\n",
      "batch 96 of size 100 out of 134.85\n",
      "batch 97 of size 100 out of 134.85\n",
      "batch 98 of size 100 out of 134.85\n",
      "batch 99 of size 100 out of 134.85\n",
      "batch 100 of size 100 out of 134.85\n",
      "batch 101 of size 100 out of 134.85\n",
      "batch 102 of size 100 out of 134.85\n",
      "batch 103 of size 100 out of 134.85\n",
      "batch 104 of size 100 out of 134.85\n",
      "batch 105 of size 100 out of 134.85\n",
      "batch 106 of size 100 out of 134.85\n",
      "batch 107 of size 100 out of 134.85\n",
      "batch 108 of size 100 out of 134.85\n",
      "batch 109 of size 100 out of 134.85\n",
      "batch 110 of size 100 out of 134.85\n",
      "batch 111 of size 100 out of 134.85\n",
      "batch 112 of size 100 out of 134.85\n",
      "batch 113 of size 100 out of 134.85\n",
      "batch 114 of size 100 out of 134.85\n",
      "batch 115 of size 100 out of 134.85\n",
      "batch 116 of size 100 out of 134.85\n",
      "batch 117 of size 100 out of 134.85\n",
      "batch 118 of size 100 out of 134.85\n",
      "batch 119 of size 100 out of 134.85\n",
      "batch 120 of size 100 out of 134.85\n",
      "batch 121 of size 100 out of 134.85\n",
      "batch 122 of size 100 out of 134.85\n",
      "batch 123 of size 100 out of 134.85\n",
      "batch 124 of size 100 out of 134.85\n",
      "batch 125 of size 100 out of 134.85\n",
      "batch 126 of size 100 out of 134.85\n",
      "batch 127 of size 100 out of 134.85\n",
      "batch 128 of size 100 out of 134.85\n",
      "batch 129 of size 100 out of 134.85\n",
      "batch 130 of size 100 out of 134.85\n",
      "batch 131 of size 100 out of 134.85\n",
      "batch 132 of size 100 out of 134.85\n",
      "batch 133 of size 100 out of 134.85\n",
      "batch 134 of size 100 out of 134.85\n",
      "total time taken this loop:  8673.039563417435\n"
     ]
    }
   ],
   "source": [
    "# Choose batch_size\n",
    "stide_len = 100\n",
    "\n",
    "my_model  = SkillsColBERT()\n",
    "my_model.to(torch.device(device))\n",
    "\n",
    "\n",
    "# Initialize tensor to store output\n",
    "d_id    = torch.tensor(d_input_ids[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "d_mask  = torch.tensor(d_attention_masks[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "doc_output = my_model(d_id, mask=d_mask)\n",
    "\n",
    "step = 0\n",
    "i = 0\n",
    "start_time = time.time()\n",
    "while step < len(df):\n",
    "    if os.path.exists(os.path.join(os.getcwd(), 'doc_embeddings', f'tensor_{i}.pt')):\n",
    "        step += stide_len\n",
    "        i += 1\n",
    "    else:\n",
    "        if step % (500)==0:\n",
    "            print(f'batch {i} of size {stide_len} out of {len(df)/stide_len}')\n",
    "\n",
    "        d_id    = torch.tensor(d_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "        d_mask  = torch.tensor(np.array(d_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "        # Find Embeddings of documents and save to disk\n",
    "        torch.save(my_model(d_id, mask=d_mask), f'./doc_embeddings/tensor_{i}.pt')\n",
    "\n",
    "        # Add stride_length to step\n",
    "        step += stide_len\n",
    "        i += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total time taken this loop: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload document embeddings\n",
    "- Reload tensors to collect them into one big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading document embedding 20\n",
      "loading document embedding 40\n",
      "loading document embedding 60\n",
      "loading document embedding 80\n",
      "loading document embedding 100\n",
      "loading document embedding 120\n"
     ]
    }
   ],
   "source": [
    "load_doc_embeddings = torch.load(f'./doc_embeddings/tensor_{0}.pt')\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(os.getcwd(), 'doc_embeddings', f'tensor_{i}.pt')):\n",
    "    if i % (20)==0:\n",
    "        print(f'loading document embedding {i}')\n",
    "    \n",
    "    load_doc_embeddings = torch.cat((load_doc_embeddings, torch.load(f'./doc_embeddings/tensor_{i}.pt')), 0)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13485, 128, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full tensor\n",
    "torch.save(load_doc_embeddings, f'./doc_embeddings/doc_tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of size 100 out of 134.85\n",
      "batch 1 of size 100 out of 134.85\n",
      "batch 2 of size 100 out of 134.85\n",
      "batch 3 of size 100 out of 134.85\n",
      "batch 4 of size 100 out of 134.85\n",
      "batch 5 of size 100 out of 134.85\n",
      "batch 6 of size 100 out of 134.85\n",
      "batch 7 of size 100 out of 134.85\n",
      "batch 8 of size 100 out of 134.85\n",
      "batch 9 of size 100 out of 134.85\n",
      "batch 10 of size 100 out of 134.85\n",
      "batch 11 of size 100 out of 134.85\n",
      "batch 12 of size 100 out of 134.85\n",
      "batch 13 of size 100 out of 134.85\n",
      "batch 14 of size 100 out of 134.85\n",
      "batch 15 of size 100 out of 134.85\n",
      "batch 16 of size 100 out of 134.85\n",
      "batch 17 of size 100 out of 134.85\n",
      "batch 18 of size 100 out of 134.85\n",
      "batch 19 of size 100 out of 134.85\n",
      "batch 20 of size 100 out of 134.85\n",
      "batch 21 of size 100 out of 134.85\n",
      "batch 22 of size 100 out of 134.85\n",
      "batch 23 of size 100 out of 134.85\n",
      "batch 24 of size 100 out of 134.85\n",
      "batch 25 of size 100 out of 134.85\n",
      "batch 26 of size 100 out of 134.85\n",
      "batch 27 of size 100 out of 134.85\n",
      "batch 28 of size 100 out of 134.85\n",
      "batch 29 of size 100 out of 134.85\n",
      "batch 30 of size 100 out of 134.85\n",
      "batch 31 of size 100 out of 134.85\n",
      "batch 32 of size 100 out of 134.85\n",
      "batch 33 of size 100 out of 134.85\n",
      "batch 34 of size 100 out of 134.85\n",
      "batch 35 of size 100 out of 134.85\n",
      "batch 36 of size 100 out of 134.85\n",
      "batch 37 of size 100 out of 134.85\n",
      "batch 38 of size 100 out of 134.85\n",
      "batch 39 of size 100 out of 134.85\n",
      "batch 40 of size 100 out of 134.85\n",
      "batch 41 of size 100 out of 134.85\n",
      "batch 42 of size 100 out of 134.85\n",
      "batch 43 of size 100 out of 134.85\n",
      "batch 44 of size 100 out of 134.85\n",
      "batch 45 of size 100 out of 134.85\n",
      "batch 46 of size 100 out of 134.85\n",
      "batch 47 of size 100 out of 134.85\n",
      "batch 48 of size 100 out of 134.85\n",
      "batch 49 of size 100 out of 134.85\n",
      "batch 50 of size 100 out of 134.85\n",
      "batch 51 of size 100 out of 134.85\n",
      "batch 52 of size 100 out of 134.85\n",
      "batch 53 of size 100 out of 134.85\n",
      "batch 54 of size 100 out of 134.85\n",
      "batch 55 of size 100 out of 134.85\n",
      "batch 56 of size 100 out of 134.85\n",
      "batch 57 of size 100 out of 134.85\n",
      "batch 58 of size 100 out of 134.85\n",
      "batch 59 of size 100 out of 134.85\n",
      "batch 60 of size 100 out of 134.85\n",
      "batch 61 of size 100 out of 134.85\n",
      "batch 62 of size 100 out of 134.85\n",
      "batch 63 of size 100 out of 134.85\n",
      "batch 64 of size 100 out of 134.85\n",
      "batch 65 of size 100 out of 134.85\n",
      "batch 66 of size 100 out of 134.85\n",
      "batch 67 of size 100 out of 134.85\n",
      "batch 68 of size 100 out of 134.85\n",
      "batch 69 of size 100 out of 134.85\n",
      "batch 70 of size 100 out of 134.85\n",
      "batch 71 of size 100 out of 134.85\n",
      "batch 72 of size 100 out of 134.85\n",
      "batch 73 of size 100 out of 134.85\n",
      "batch 74 of size 100 out of 134.85\n",
      "batch 75 of size 100 out of 134.85\n",
      "batch 76 of size 100 out of 134.85\n",
      "batch 77 of size 100 out of 134.85\n",
      "batch 78 of size 100 out of 134.85\n",
      "batch 79 of size 100 out of 134.85\n",
      "batch 80 of size 100 out of 134.85\n",
      "batch 81 of size 100 out of 134.85\n",
      "batch 82 of size 100 out of 134.85\n",
      "batch 83 of size 100 out of 134.85\n",
      "batch 84 of size 100 out of 134.85\n",
      "batch 85 of size 100 out of 134.85\n",
      "batch 86 of size 100 out of 134.85\n",
      "batch 87 of size 100 out of 134.85\n",
      "batch 88 of size 100 out of 134.85\n",
      "batch 89 of size 100 out of 134.85\n",
      "batch 90 of size 100 out of 134.85\n",
      "batch 91 of size 100 out of 134.85\n",
      "batch 92 of size 100 out of 134.85\n",
      "batch 93 of size 100 out of 134.85\n",
      "batch 94 of size 100 out of 134.85\n",
      "batch 95 of size 100 out of 134.85\n",
      "batch 96 of size 100 out of 134.85\n",
      "batch 97 of size 100 out of 134.85\n",
      "batch 98 of size 100 out of 134.85\n",
      "batch 99 of size 100 out of 134.85\n",
      "batch 100 of size 100 out of 134.85\n",
      "batch 101 of size 100 out of 134.85\n",
      "batch 102 of size 100 out of 134.85\n",
      "batch 103 of size 100 out of 134.85\n",
      "batch 104 of size 100 out of 134.85\n",
      "batch 105 of size 100 out of 134.85\n",
      "batch 106 of size 100 out of 134.85\n",
      "batch 107 of size 100 out of 134.85\n",
      "batch 108 of size 100 out of 134.85\n",
      "batch 109 of size 100 out of 134.85\n",
      "batch 110 of size 100 out of 134.85\n",
      "batch 111 of size 100 out of 134.85\n",
      "batch 112 of size 100 out of 134.85\n",
      "batch 113 of size 100 out of 134.85\n",
      "batch 114 of size 100 out of 134.85\n",
      "batch 115 of size 100 out of 134.85\n",
      "batch 116 of size 100 out of 134.85\n",
      "batch 117 of size 100 out of 134.85\n",
      "batch 118 of size 100 out of 134.85\n",
      "batch 119 of size 100 out of 134.85\n",
      "batch 120 of size 100 out of 134.85\n",
      "batch 121 of size 100 out of 134.85\n",
      "batch 122 of size 100 out of 134.85\n",
      "batch 123 of size 100 out of 134.85\n",
      "batch 124 of size 100 out of 134.85\n",
      "batch 125 of size 100 out of 134.85\n",
      "batch 126 of size 100 out of 134.85\n",
      "batch 127 of size 100 out of 134.85\n",
      "batch 128 of size 100 out of 134.85\n",
      "batch 129 of size 100 out of 134.85\n",
      "batch 130 of size 100 out of 134.85\n",
      "batch 131 of size 100 out of 134.85\n",
      "batch 132 of size 100 out of 134.85\n",
      "batch 133 of size 100 out of 134.85\n",
      "batch 134 of size 100 out of 134.85\n",
      "total time taken this loop:  8673.039563417435\n"
     ]
    }
   ],
   "source": [
    "# Choose batch_size\n",
    "stide_len = 100\n",
    "\n",
    "my_model  = SkillsColBERT()\n",
    "my_model.to(torch.device(device))\n",
    "\n",
    "d_mask\n",
    "# Initialize tensor to store output\n",
    "q_id    = torch.tensor(q_input_ids[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "q_mask  = torch.tensor(q_attention_masks[:stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "doc_output = my_model(q_id, mask=q_mask)\n",
    "\n",
    "step = 0\n",
    "i = 0\n",
    "start_time = time.time()\n",
    "while step < len(df):\n",
    "    if os.path.exists(os.path.join(os.getcwd(), 'doc_embeddings', f'tensor_{i}.pt')):\n",
    "        step += stide_len\n",
    "        i += 1\n",
    "    else:\n",
    "        if step % (500)==0:\n",
    "            print(f'batch {i} of size {stide_len} out of {len(df)/stide_len}')\n",
    "\n",
    "        q_id    = torch.tensor(q_input_ids[step:step+stide_len]).to(torch.device(device)).to(torch.int64)\n",
    "        q_mask  = torch.tensor(np.array(q_attention_masks[step:step+stide_len])).to(torch.device(device)).to(torch.int64)\n",
    "\n",
    "        # Find Embeddings of documents and save to disk\n",
    "        torch.save(my_model(q_id, mask=q_mask), f'./query_embeddings/tensor_{i}.pt')\n",
    "\n",
    "        # Add stride_length to step\n",
    "        step += stide_len\n",
    "        i += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total time taken this loop: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload document embeddings\n",
    "- Reload all batched tensors to collect them into one big and save that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading document embedding 20\n",
      "loading document embedding 40\n",
      "loading document embedding 60\n",
      "loading document embedding 80\n",
      "loading document embedding 100\n"
     ]
    }
   ],
   "source": [
    "load_query_embeddings = torch.load(f'./query_embeddings/tensor_{0}.pt')\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(os.getcwd(), 'query_embeddings', f'tensor_{i}.pt')):\n",
    "    if i % (20)==0:\n",
    "        print(f'loading document embedding {i}')\n",
    "    \n",
    "    load_query_embeddings = torch.cat((load_query_embeddings, torch.load(f'./query_embeddings/tensor_{i}.pt')), 0)\n",
    "    i += 1\n",
    "\n",
    "print('Last batch run: ', i)\n",
    "print('Shape of :', load_query_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full tensor  (5 GB)\n",
    "torch.save(load_query_embeddings, f'./query_embeddings/query_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62490302a320790e9096d978396a0f6884d50306ab9199b7a47371992da1d123"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('colbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
